[yuying_chameleon]
# API 提供方（用于自动填充合理默认值；具体仍以 base_url/model 配置为准）
# 支持分别为 主 LLM / 便宜 LLM / Embedder 指定不同提供商：
# - openai: OpenAI 官方/兼容网关
# - ark: 火山方舟 OpenAI 兼容网关
api_provider = "openai"        # 全局默认（未配置独立 provider 时生效）
main_provider = ""             # 可选：主 LLM provider，留空则用 api_provider
cheap_llm_provider = ""        # 可选：便宜 LLM provider，留空则用 api_provider
embedder_provider = ""         # 可选：Embedding provider，留空则用 api_provider

# 机器人基础设置
superusers = ["123456789"]
nickname = ["YuYing"]

# 数据库设置
database_url = "sqlite+aiosqlite:///data/yuying.db"
# 说明：SQLite 的 PRAGMA 相关设置在引擎初始化时按设计写死
sqlite_busy_timeout_ms = 3000

# Qdrant 向量库设置
qdrant_host = "localhost"
qdrant_port = 6333
qdrant_api_key = ""       # 可选
qdrant_https = false      # 可选：本地一般为 false，Qdrant Cloud 一般为 true
qdrant_recreate_collections = false  # 可选：维度变化时自动删除并重建（有数据丢失风险）

# 索引后台任务并发（向量化 + 写入 Qdrant）
# - 默认 1（串行最稳）；并发过高可能触发 embedding/Qdrant 限流
index_worker_max_concurrency = 1

# 表情包打标签后台任务并发（LLM + 图片）
# - 默认 1（串行最稳）；并发过高可能触发 LLM 限流
sticker_worker_max_concurrency = 1
retrieval_topk = 5
retrieval_snippet_max_chars = 120
hybrid_query_recent_messages_limit = 30      # Hybrid Query 读取场景最近消息条数
hybrid_query_recent_user_messages_limit = 3  # Hybrid Query 中用户最近消息条数
recent_dialogue_max_lines = 30               # 注入到主 prompt 的最近对话行数

# OpenAI / LLM 设置
openai_base_url = "https://api.openai.com/v1"
openai_api_key = "sk-..."
openai_model = "gpt-4-turbo"
openai_timeout = 30.0

# OpenAI Python SDK 默认请求头（可选）
# - 作用: 透传给 openai.AsyncOpenAI(default_headers=...)，常用于设置 User-Agent
# - 写法: TOML inline table（key 如含 '-' 建议加引号）
# - 示例: openai_default_headers = { "User-Agent" = "YuYing-Chameleon/1.0" }
# openai_default_headers = { "User-Agent" = "YuYing-Chameleon/1.0" }

cheap_llm_base_url = "https://api.openai.com/v1"
cheap_llm_api_key = "sk-..."
cheap_llm_model = "gpt-3.5-turbo"
cheap_llm_timeout = 10.0

embedder_base_url = "https://api.openai.com/v1"
embedder_api_key = "sk-..."
embedder_model = "text-embedding-3-small"
embedder_endpoint = "/embeddings"     # 可选：火山方舟多模态为 `/embeddings/multimodal`（但文本索引建议用 `/embeddings` + 文本向量模型）
embedder_timeout = 30.0              # 可选：embedding 请求超时（秒）
vector_size = 2048                   # 可选：向量维度（需与 embedding 输出一致）

# 媒体理解设置（图片说明/OCR）
# 说明：
# - 图片说明/OCR 完全复用 cheap_llm_* 配置（需要支持图片输入的多模态模型）；
# - 若图片任务更易超时/更慢，请调大 cheap_llm_timeout（图片任务不再使用独立超时配置）。
media_enable_ocr = false             # 可选：图片预处理是否做 OCR（默认关闭，仅 caption）

# 记忆设置
memory_effective_count_threshold = 50
memory_condense_hour = 3              # 凌晨 03:00
memory_core_limit = 20
memory_active_ttl_days = 7
memory_idle_seconds = 120
memory_overlap_messages = 10
memory_extract_max_messages = 60
memory_archive_days = 90

# AI 主动记忆写入速率限制（防止滥用）
# 说明：
# - AI 可在对话中主动调用 create_memory 工具写入重要记忆
# - 与定时批量提取（凌晨 3 点）并行，形成双轨记忆系统
# - 速率限制分为会话级和每日级，分群聊和私聊
memory_session_limit_group = 3        # 群聊每会话限额（条）
memory_session_limit_private = 5      # 私聊每会话限额（条）
memory_daily_limit_group = 25         # 群聊每日限额（条/天）
memory_daily_limit_private = 40       # 私聊每日限额（条/天）
memory_session_idle_timeout = 600.0   # 会话空闲超时（秒，10分钟）

# 表情包设置
sticker_promote_threshold = 3        # 候选表情包晋升阈值（使用次数）
sticker_cooldown_seconds = 60        # 表情包使用冷却时间（秒）
sticker_meme_score_threshold = 3     # 表情包趣味性评分阈值（1-10分）
sticker_use_semantic_search = true   # 是否启用语义检索（向量匹配）
sticker_vector_top_k = 50            # 向量召回候选数量（topK）

# Nano 模型设置（用于心流模式的前置决策）
nano_llm_base_url = "https://api.openai.com/v1"
nano_llm_api_key = "sk-..."
nano_llm_model = "gpt-4o-mini"
nano_llm_timeout = 5.0
nano_llm_provider = ""  # 可选：nano LLM provider，留空则用 api_provider

# 回复策略设置
global_cooldown_seconds = 30
group_cooldown_seconds = 60
group_reply_probability = 0.15
private_reply_probability = 1.0
spam_window_seconds = 30
spam_msg_threshold = 12
action_max_count = 4
action_min_delay_ms = 300
action_max_delay_ms = 900
reply_text_max_chars = 40

# 心流模式设置
# 说明：心流模式使用 nano 模型作为前置判断，决定是否回复每条消息
# 优点：更智能的回复决策，能理解上下文
# 缺点：会增加模型消耗和处理延迟
enable_flow_mode = false  # 是否启用心流模式
flow_mode_global_cooldown_seconds = 30  # 心流模式下的全局冷却时间
flow_mode_group_cooldown_seconds = 60  # 心流模式下的群聊冷却时间
flow_mode_group_check_probability = 0.8  # 心流模式下群聊消息被检测的概率（80%）
flow_mode_private_check_probability = 1.0  # 心流模式下私聊消息被检测的概率（100%）

# 摘要设置
summary_window_message_count = 20
summary_window_seconds = 900

# 启用自适应防抖
adaptive_debounce_enabled = true

# 可选：调整参数（以下为默认值，可以不写）
adaptive_debounce_joiner = "auto"         # 拼接策略
adaptive_debounce_max_hold_seconds = 15.0 # 硬截止时间
adaptive_debounce_max_parts = 12          # 最多拼接段数
adaptive_debounce_max_plain_len = 300     # 最大纯文本长度

# 可选：调整公式参数（谨慎修改）
adaptive_debounce_w1 = 0.6                # 一次项系数
adaptive_debounce_w2 = -0.025             # 二次项系数
adaptive_debounce_w3 = -2.5               # 标点符号系数
adaptive_debounce_bias = 1.5              # 基础等待时间
adaptive_debounce_min_wait = 0.5          # 最小等待时间
adaptive_debounce_max_wait = 5.0          # 最大等待时间


# ==================== MCP（Model Context Protocol）工具集成 ====================
# 说明：
# - MCP 用于将外部工具（搜索/文件/数据库等）以统一协议接入 LLM
# - 启用后，LLM 可通过 OpenAI function calling 机制调用配置的工具
# - 向后兼容：enable_mcp=false 时完全不影响现有功能
# - 依赖安装：`uv pip install .[mcp]` 或 `pip install .[mcp]`

enable_mcp = false  # 是否启用 MCP（默认关闭）

# MCP 运行策略
mcp_lazy_connect = true  # 懒加载：启动时不连接 server，首次使用时才连接（推荐）
mcp_fail_open = true     # 失败降级：MCP 失败不影响主流程，继续走无工具路径（推荐）

# MCP 工具调用限制
mcp_tool_timeout = 15.0           # 单次工具调用超时（秒），避免工具卡死
mcp_max_tool_calls = 6            # 单次规划最大工具调用次数（防止无限循环）
mcp_tool_result_max_chars = 2000  # 工具返回结果最大字符数（截断以控制 token 成本）

# MCP 并发策略
mcp_parallel_tools = false     # 同一轮多个工具调用是否并发执行（默认串行更安全）
mcp_max_parallel_tools = 4     # 并发执行的最大并行度（仅在 parallel_tools=true 时生效）

# MCP Server 配置列表（支持多个 server 并存）
# 格式：[[yuying_chameleon.mcp_servers]] 开启一个新 server 配置块

# 示例 1: 文件系统工具（本地 Python 模块）
# [[yuying_chameleon.mcp_servers]]
# id = "filesystem"                  # 必填：server 唯一标识（不同 server 的 id 不能重复）
# enabled = true                     # 可选：是否启用此 server（默认 true）
# display_name = "文件系统"           # 可选：显示名称，会写入工具 description 的 [Name] 前缀
# transport = "stdio"                # 传输方式：首期只支持 stdio（通过子进程 stdin/stdout 通信）
# command = "python"                 # 必填：启动 MCP server 的命令
# args = ["-m", "mcp_server_filesystem"]  # 可选：命令参数
# cwd = "/path/to/workspace"         # 可选：工作目录
# env = { "LOG_LEVEL" = "INFO" }     # 可选：环境变量（会继承系统环境并 merge）

# 工具过滤（可选，用于控制 token 成本）
# allow_tools = ["read_file", "list_directory"]  # 白名单：仅允许这些工具（优先级高）
# deny_tools = ["delete_file"]                   # 黑名单：禁止这些工具

# 示例 2: Web 搜索工具（Node.js MCP server）
# [[yuying_chameleon.mcp_servers]]
# id = "brave-search"
# display_name = "Brave搜索"
# transport = "stdio"
# command = "node"
# args = ["/path/to/brave-search-mcp/index.js"]
# env = { "BRAVE_API_KEY" = "your-api-key-here" }

# 示例 3: 数据库查询工具
# [[yuying_chameleon.mcp_servers]]
# id = "postgres"
# display_name = "数据库"
# transport = "stdio"
# command = "npx"
# args = ["-y", "@modelcontextprotocol/server-postgres", "postgresql://user:pass@localhost/dbname"]
# allow_tools = ["query", "list_tables"]  # 只允许查询和列表，禁止写入操作

# 注意事项：
# 1. 工具名冲突：多个 server 提供同名工具时，会自动加 `__{server_id}` 后缀
# 2. Schema 转换：MCP inputSchema 会自动转换为 OpenAI parameters，复杂 schema 可能被降级
# 3. 安全性：工具有完整的系统访问权限，请只配置可信的 MCP server
# 4. 调试：启用 MCP 后可查看日志中的 "MCP tools 已加载: X 个" 确认工具注册成功

# ==================== LLM 模型组与任务路由配置 ====================
# 说明：
# - 此配置为高级功能，支持模型组（多模型 fallback）和任务级别模型选择
# - 不配置此段时，将使用上方的旧配置（openai_*, cheap_llm_*, nano_llm_*）
# - 配置此段后，可实现：
#   1. 模型组：配置多个模型，失败时自动切换下一个
#   2. 任务路由：不同任务使用不同模型（如摘要用便宜模型，对话用高级模型）
#   3. 向下兼容：未配置的组/任务仍使用旧配置

[yuying_chameleon.llm]

# 全局策略（可选）
[yuying_chameleon.llm.policy]
per_model_attempts = 1              # 每个模型最多尝试次数（默认1，不重试）
total_attempts_cap = 5              # 单次调用最多总尝试次数（防止模型组太长）
treat_empty_as_failure = true       # content=="" 且无 tool_calls 视为失败
base_backoff_seconds = 0.3          # 重试退避基础值（秒，带 jitter）

# 主模型组（用于主对话、动作规划等核心任务）
# 方式1：单模型
[yuying_chameleon.llm.main]
model = "gpt-4-turbo"

# 方式2：模型组（按顺序 fallback）
# models = ["gpt-4-turbo", "claude-3-opus"]

# 便宜模型组（用于摘要、标签生成等后台任务）
# 四种配置方式任选其一：

# 方式1：使用统一供应商配置（推荐，适合同一供应商的多个模型）
[yuying_chameleon.llm.cheap]
base_url = "https://api.deepseek.com/v1"  # 模型组级别的 base_url
api_key = "sk-deepseek-xxx"                # 模型组级别的 api_key
timeout = 12.0                             # 模型组级别的 timeout
models = ["deepseek-chat", "deepseek-coder"]  # 都使用上面的 DeepSeek 配置，失败时自动切换

# 方式2：混合多个供应商（旧格式，适合跨供应商 fallback）
# [yuying_chameleon.llm.cheap]
# base_url = "https://api.deepseek.com/v1"  # 模型组默认使用 DeepSeek
# api_key = "sk-deepseek-xxx"
# timeout = 12.0
# models = [
#   "deepseek-chat",  # 使用模型组的 DeepSeek 配置
#   { model = "gpt-3.5-turbo", base_url = "https://api.openai.com/v1", api_key = "sk-openai-xxx", timeout = 10.0 }  # 内联覆盖为 OpenAI
# ]

# 方式3：供应商数组（新格式，推荐！结构最清晰）
# [yuying_chameleon.llm.cheap]
# providers = [
#   { name = "deepseek", base_url = "https://api.deepseek.com/v1", api_key = "sk-deepseek-xxx", timeout = 12.0, models = ["deepseek-chat", "deepseek-coder"] },
#   { name = "openai", base_url = "https://api.openai.com/v1", api_key = "sk-openai-xxx", timeout = 10.0, models = ["gpt-3.5-turbo", "gpt-4o-mini"] }
# ]

# 方式4：仅配置模型列表（使用上方旧配置的 cheap_llm_* 参数）
# [yuying_chameleon.llm.cheap]
# models = ["gpt-3.5-turbo", "claude-3-haiku"]

# Nano 模型组（用于心流模式的快速决策）
[yuying_chameleon.llm.nano]
model = "gpt-4o-mini"

# 任务级别路由（可选，不配置则使用默认映射）
# 值可以是：模型组名（main/cheap/nano）或 直接的模型名
# 说明：所有模型都支持多模态，图片相关任务直接使用 main 或 cheap 即可
[yuying_chameleon.llm.tasks]
action_planner = "main"              # 主对话/动作规划
memory_extraction = "cheap"          # 记忆提取
memory_condenser = "main"            # 记忆浓缩
summary_generation = "cheap"         # 摘要生成
sticker_tagging = "cheap"            # 表情包标签
flow_decider = "nano"                # 心流模式决策
vision_caption = "main"              # 图片说明（所有模型都支持多模态）
vision_ocr = "main"                  # OCR（所有模型都支持多模态）
personality_reflection = "cheap"     # 人格反思（recent 层）
personality_core = "main"            # 人格核心原则更新（core 层）

# 示例：直接指定模型名（而非模型组）
# memory_condenser = "gpt-4"         # 直接使用 gpt-4，不走模型组
